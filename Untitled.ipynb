{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d9a98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File wikipedia_documents.csv already exists. Skipping scraping.\n",
      "Documents found: ['1']\n",
      "Evaluation Results - Precision: 1.0, Recall: 0.3333333333333333, F1-score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Wikipedia Search Engine Notebook\n",
    "\n",
    "## Εισαγωγή\n",
    "# Σε αυτό το Notebook, υλοποιούμε μία μηχανή αναζήτησης που βασίζεται σε δεδομένα από τη Wikipedia.\n",
    "# Ο κώδικας περιλαμβάνει:\n",
    "# 1. Συλλογή δεδομένων από τη Wikipedia.\n",
    "# 2. Επεξεργασία κειμένου και δημιουργία αντιστραμμένου ευρετηρίου.\n",
    "# 3. Υλοποίηση βασικών λειτουργιών αναζήτησης.\n",
    "# 4. Αξιολόγηση της μηχανής αναζήτησης.\n",
    "\n",
    "## Βιβλιοθήκες και Ρυθμίσεις\n",
    "# Εδώ εισάγουμε τις απαραίτητες βιβλιοθήκες για τη συλλογή και επεξεργασία των δεδομένων.\n",
    "# Κατεβάζουμε επίσης τα απαραίτητα δεδομένα για την επεξεργασία κειμένου.\n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Κατεβάζουμε τα απαραίτητα δεδομένα για την επεξεργασία κειμένου\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "## Υλοποίηση Κλάσης WikipediaScraper\n",
    "# Σε αυτό το τμήμα, ορίζουμε την κλάση `WikipediaScraper`, η οποία συλλέγει δεδομένα από άρθρα της Wikipedia βάσει θεμάτων.\n",
    "# Τα δεδομένα αποθηκεύονται σε ένα αρχείο CSV για περαιτέρω επεξεργασία.\n",
    "\n",
    "class WikipediaScraper:\n",
    "    \"\"\"Class για συλλογή δεδομένων από την Wikipedia.\"\"\"\n",
    "\n",
    "    def __init__(self, base_url, csv_file):\n",
    "        self.base_url = base_url\n",
    "        self.csv_file = csv_file\n",
    "\n",
    "    def scrape(self, topic_list):\n",
    "        \"\"\"Συλλέγει άρθρα από τη Wikipedia βάσει των θεμάτων και τα αποθηκεύει σε CSV.\"\"\"\n",
    "        if os.path.exists(self.csv_file):\n",
    "            print(f\"File {self.csv_file} already exists. Skipping scraping.\")\n",
    "            return\n",
    "\n",
    "        with open(self.csv_file, 'w', encoding='utf-8', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['doc_id', 'content'])\n",
    "            doc_id = 1\n",
    "\n",
    "            for topic in topic_list:\n",
    "                url = f\"{self.base_url}{topic}\"\n",
    "                print(f\"Scraping: {url}\")\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    paragraphs = soup.find_all('p')\n",
    "                    content = \" \".join([para.get_text() for para in paragraphs])\n",
    "                    if content.strip():\n",
    "                        writer.writerow([doc_id, content])\n",
    "                        doc_id += 1\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve: {url}\")\n",
    "\n",
    "        print(f\"Scraping complete. Data saved to {self.csv_file}\")\n",
    "\n",
    "## Υλοποίηση Κλάσης SearchEngine\n",
    "# Στο επόμενο βήμα, υλοποιούμε την κλάση `SearchEngine`, η οποία περιλαμβάνει μεθόδους για την αναζήτηση και επεξεργασία κειμένων.\n",
    "\n",
    "class SearchEngine:\n",
    "    \"\"\"Class για αναζήτηση εγγράφων.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.csv_file = csv_file\n",
    "        self.documents = {}\n",
    "        self.inverted_index = defaultdict(list)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self._load_documents()\n",
    "        self._build_inverted_index()\n",
    "\n",
    "    def _load_documents(self):\n",
    "        \"\"\"Φορτώνει έγγραφα από το αρχείο CSV.\"\"\"\n",
    "        if not os.path.exists(self.csv_file):\n",
    "            raise FileNotFoundError(f\"File {self.csv_file} does not exist.\")\n",
    "\n",
    "        with open(self.csv_file, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                doc_id, content = row[0], row[1]\n",
    "                self.documents[doc_id] = content\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"Επεξεργάζεται και κανονικοποιεί το κείμενο.\"\"\"\n",
    "        text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [token for token in tokens if token not in self.stop_words and token not in string.punctuation]\n",
    "        tokens = [self.stemmer.stem(self.lemmatizer.lemmatize(token)) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def _build_inverted_index(self):\n",
    "        \"\"\"Δημιουργεί το inverted index από τα έγγραφα.\"\"\"\n",
    "        for doc_id, content in self.documents.items():\n",
    "            tokens = self._preprocess(content)\n",
    "            for token in set(tokens):\n",
    "                self.inverted_index[token].append(doc_id)\n",
    "\n",
    "    def search(self, query):\n",
    "        \"\"\"Αναζητά έγγραφα που ταιριάζουν με το ερώτημα.\"\"\"\n",
    "        query_tokens = self._preprocess(query)\n",
    "        if not query_tokens:\n",
    "            return []\n",
    "\n",
    "        doc_scores = defaultdict(int)\n",
    "        for token in query_tokens:\n",
    "            if token in self.inverted_index:\n",
    "                for doc_id in self.inverted_index[token]:\n",
    "                    doc_scores[doc_id] += 1\n",
    "\n",
    "        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [doc_id for doc_id, _ in sorted_docs]\n",
    "\n",
    "    def evaluate(self, test_queries, test_labels):\n",
    "        \"\"\"Αξιολογεί τη μηχανή αναζήτησης με precision, recall, και F1-score.\"\"\"\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for query, relevant_docs in zip(test_queries, test_labels):\n",
    "            retrieved_docs = self.search(query)\n",
    "            y_true.extend([1 if doc in relevant_docs else 0 for doc in self.documents.keys()])\n",
    "            y_pred.extend([1 if doc in retrieved_docs else 0 for doc in self.documents.keys()])\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        return precision, recall, f1\n",
    "\n",
    "## Παράδειγμα Χρήσης\n",
    "# Εδώ παρουσιάζουμε ένα παράδειγμα χρήσης της μηχανής αναζήτησης, περιλαμβάνοντας scraping, αναζήτηση και αξιολόγηση.\n",
    "\n",
    "### Είσοδος Θεμάτων\n",
    "# Εισάγουμε τα θέματα που θέλουμε να συλλέξουμε από τη Wikipedia.\n",
    "user_input = \"Natural_language_processing,Python_(programming_language)\"\n",
    "topics = [topic.strip() for topic in user_input.split(',') if topic.strip()]\n",
    "\n",
    "### Scraping\n",
    "# Εκτελούμε τη διαδικασία scraping για να συλλέξουμε τα δεδομένα.\n",
    "scraper = WikipediaScraper(\"https://en.wikipedia.org/wiki/\", \"wikipedia_documents.csv\")\n",
    "scraper.scrape(topics)\n",
    "\n",
    "### Αναζήτηση\n",
    "# Δημιουργούμε αντικείμενο SearchEngine και πραγματοποιούμε αναζήτηση για το ερώτημα \"machine learning\".\n",
    "search_engine = SearchEngine('wikipedia_documents.csv')\n",
    "query = \"machine learning\"\n",
    "results = search_engine.search(query)\n",
    "print(\"Documents found:\", results)\n",
    "\n",
    "### Αξιολόγηση\n",
    "# Αξιολογούμε τη μηχανή αναζήτησης χρησιμοποιώντας test queries και labels.\n",
    "test_queries = [\"machine learning\", \"neural networks\"]\n",
    "test_labels = [[\"1\", \"2\"], [\"3\", \"4\"]]\n",
    "precision, recall, f1 = search_engine.evaluate(test_queries, test_labels)\n",
    "print(f\"Evaluation Results - Precision: {precision}, Recall: {recall}, F1-score: {f1}\")\n",
    "\n",
    "# Τέλος Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e436e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291589bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
